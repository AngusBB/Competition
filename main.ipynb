{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# pre_Data-Format\n",
    "### Step_1. FormatOfficial2Yolo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Renaming Training Dataset_v5 --> Origin_Training_Dataset\r\n",
      "2. Backing-up Labels to Dataset/Origin_Training_Dataset/origin_labels/...\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 891.24it/s]\r\n",
      "3. Formatting Official Labels to yolo Type...\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:42<00:00, 23.30it/s]\r\n",
      "4. Moving Labels to Dataset/Origin_Training_Dataset/labels...\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1710.07it/s]\r\n",
      "5. Moving images to Dataset/Origin_Training_Dataset/images...\r\n",
      "6. Start Splitting 10% of Training Data to Val Data...\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 842.31it/s]\r\n"
     ]
    }
   ],
   "source": [
    "!python Code/pre_FormatOfficial2Yolo.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# train_Model-Baseline\n",
    "### Step_2-1. Baseline model train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:\r\n",
      "*****************************************\r\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "*****************************************\r\n",
      "Unknown option: -C\r\n",
      "usage: git [--version] [--help] [-c name=value]\r\n",
      "           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n",
      "           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n",
      "           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n",
      "           <command> [<args>]\r\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mweights=yolov5/yolov5l6.pt, cfg=yolov5/yolov5l6.yaml, data=yolov5/dataset-Origin_Training_Dataset.yaml, hyp=yolov5/hyp.none.yaml, epochs=1000, batch_size=8, imgsz=1920, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=0,1, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=50, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\r\n",
      "\u001B[34m\u001B[1mgithub: \u001B[0mup to date with https://github.com/ultralytics/yolov5 âœ…\r\n",
      "Unknown option: -C\r\n",
      "usage: git [--version] [--help] [-c name=value]\r\n",
      "           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n",
      "           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n",
      "           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n",
      "           <command> [<args>]\r\n",
      "YOLOv5 ðŸš€ 2022-12-14 Python-3.8.15 torch-1.13.0+cu117 CUDA:0 (NVIDIA A100-PCIE-40GB, 40536MiB)\r\n",
      "                                                      CUDA:1 (NVIDIA A100-PCIE-40GB, 40536MiB)\r\n",
      "\r\n",
      "\u001B[34m\u001B[1mhyperparameters: \u001B[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, degrees=0.0, translate=0.0, scale=0.0, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.0, mixup=0.0, copy_paste=0.0\r\n",
      "\u001B[34m\u001B[1mClearML: \u001B[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 ðŸš€ in ClearML\r\n",
      "\u001B[34m\u001B[1mComet: \u001B[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\r\n",
      "\u001B[34m\u001B[1mTensorBoard: \u001B[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\r\n",
      "\r\n",
      "                 from  n    params  module                                  arguments                     \r\n",
      "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \r\n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \r\n",
      "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \r\n",
      "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \r\n",
      "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \r\n",
      "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \r\n",
      "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \r\n",
      "  7                -1  1   3540480  models.common.Conv                      [512, 768, 3, 2]              \r\n",
      "  8                -1  3   5611008  models.common.C3                        [768, 768, 3]                 \r\n",
      "  9                -1  1   7079936  models.common.Conv                      [768, 1024, 3, 2]             \r\n",
      " 10                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \r\n",
      " 11                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \r\n",
      " 12                -1  1    787968  models.common.Conv                      [1024, 768, 1, 1]             \r\n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n",
      " 14           [-1, 8]  1         0  models.common.Concat                    [1]                           \r\n",
      " 15                -1  3   6200832  models.common.C3                        [1536, 768, 3, False]         \r\n",
      " 16                -1  1    394240  models.common.Conv                      [768, 512, 1, 1]              \r\n",
      " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n",
      " 18           [-1, 6]  1         0  models.common.Concat                    [1]                           \r\n",
      " 19                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \r\n",
      " 20                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \r\n",
      " 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n",
      " 22           [-1, 4]  1         0  models.common.Concat                    [1]                           \r\n",
      " 23                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \r\n",
      " 24                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \r\n",
      " 25          [-1, 20]  1         0  models.common.Concat                    [1]                           \r\n",
      " 26                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \r\n",
      " 27                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \r\n",
      " 28          [-1, 16]  1         0  models.common.Concat                    [1]                           \r\n",
      " 29                -1  3   5807616  models.common.C3                        [1024, 768, 3, False]         \r\n",
      " 30                -1  1   5309952  models.common.Conv                      [768, 768, 3, 2]              \r\n",
      " 31          [-1, 12]  1         0  models.common.Concat                    [1]                           \r\n",
      " 32                -1  3  10496000  models.common.C3                        [1536, 1024, 3, False]        \r\n",
      " 33  [23, 26, 29, 32]  1     69228  models.yolo.Detect                      [4, [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], [256, 512, 768, 1024]]\r\n",
      "YOLOv5l6 summary: 477 layers, 76185580 parameters, 76185580 gradients, 110.5 GFLOPs\r\n",
      "\r\n",
      "Transferred 786/795 items from yolov5/yolov5l6.pt\r\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mchecks passed âœ…\r\n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m SGD(lr=0.01) with parameter groups 131 weight(decay=0.0), 135 weight(decay=0.0005), 135 bias\r\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning /home/student/yueh_huangje_2022/Competition/Dataset/Origin_Train\u001B[0m\r\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mWARNING âš ï¸ /home/student/yueh_huangje_2022/Competition/Dataset/Origin_Training_Dataset/images/train/img0914.png: 1 duplicate labels removed\r\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mNew cache created: /home/student/yueh_huangje_2022/Competition/Dataset/Origin_Training_Dataset/labels/train.cache\r\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning /home/student/yueh_huangje_2022/Competition/Dataset/Origin_Trainin\u001B[0m\r\n",
      "\u001B[34m\u001B[1mval: \u001B[0mNew cache created: /home/student/yueh_huangje_2022/Competition/Dataset/Origin_Training_Dataset/labels/val.cache\r\n",
      "\r\n",
      "\u001B[34m\u001B[1mAutoAnchor: \u001B[0m3.75 anchors/target, 0.997 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\r\n",
      "Plotting labels to yolov5/runs/train/exp/labels.jpg... \r\n",
      "Image sizes 1920 train, 1920 val\r\n",
      "Using 8 dataloader workers\r\n",
      "Logging results to \u001B[1myolov5/runs/train/exp\u001B[0m\r\n",
      "Starting training for 1000 epochs...\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\r\n",
      "      0/999      20.4G      0.075     0.1416    0.02492         91       1920: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all        100       5055      0.106      0.304      0.136      0.051\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\r\n",
      "      1/999      22.2G      0.062     0.1046    0.01585         91       1920: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all        100       5055      0.364      0.289       0.14     0.0413\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\r\n",
      "      2/999      22.2G    0.06143    0.09844    0.01326        149       1920: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all        100       5055      0.393      0.305      0.146     0.0335\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\r\n",
      "      3/999      22.2G    0.05613    0.09446    0.01177         80       1920: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all        100       5055      0.317      0.557      0.364      0.129\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\r\n",
      "      4/999      22.2G    0.04819    0.08783    0.01117         17       1920: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all        100       5055      0.465      0.556      0.465       0.21\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\r\n",
      "      5/999      22.2G    0.04761    0.08872    0.00937        192       1920:  "
     ]
    }
   ],
   "source": [
    "# move the code we change in YOLOv5\n",
    "!cp Code/yolov5l6.yaml yolov5/yolov5l6.yaml\n",
    "!cp Code/dataset-Origin_Training_Dataset.yaml yolov5/dataset-Origin_Training_Dataset.yaml\n",
    "!cp Code/hyp.none.yaml yolov5/hyp.none.yaml\n",
    "\n",
    "# training\n",
    "!python -m torch.distributed.run --nproc_per_node 2 yolov5/train.py --weights yolov5/yolov5l6.pt --cfg yolov5/yolov5l6.yaml --hyp yolov5/hyp.none.yaml --data yolov5/dataset-Origin_Training_Dataset.yaml --epochs 1000 --batch-size 8 --imgsz 1920 --device 0,1 --patience 50"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step_2-2. Baseline model detect (Public 0.70)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python yolov5/detect.py --data yolov5/dataset-Origin_Training_Dataset.yaml --imgsz 1080 1920 --save-txt --save-conf --weight yolov5/runs/train/exp/weights/best.pt --source Dataset/public\n",
    "# If you skip training you can simply run the following line\n",
    "# !python yolov5/detect.py --data yolov5/dataset-Origin_Training_Dataset.yaml --imgsz 1080 1920 --save-txt --save-conf --weight Dataset/Models/baseline.pt --source Dataset/public"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python Code/post_FormatYolo2Official.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pre_Data-Cleaning\n",
    "### Step_3. Analyze bbox\n",
    "### Mistaken bbox"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python Code/pre_AnalyzeBBox_Mistaken.py\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.axis('off')\n",
    "plt.imshow(cv2.imread('Output/AnalyzeBBox_Mistaken.png'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outlier bbox\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python Code/pre_AnalyzeBBox_Outlier.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pre_Data-Classify\n",
    "### Step_4. Classify bbox with size\n",
    "### K-means"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python Code/pre_AnalyzeBBox_kmeans.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "Classes = ['car', 'hov', 'person', 'motorcycle']\n",
    "\n",
    "for Class in Classes:\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(cv2.imread(f'Output/AnalyzeBBox_3-means_{Class}.png'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pre_Data-Cleaning\n",
    "### Step_5. Compare GT bbox with baseline prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Just simply download the labels we already re-labeled.\n",
    "\n",
    "# How we re-labeled:\n",
    "# 1. Detect training dataset with baseline.pt\n",
    "# 2. Preserve the prediction's boundingboxes which have an IoU > 0.5 boundingbox in origin training dataset\n",
    "#    (why? Because the prediction boundingboxes' quality is better than human-labelling, which can avoid being insufficient or exceed the object seriously.)\n",
    "# 3. Compare remaining prediction's boundingboxes one-by-one with my human eye"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pre_Data-SuperResolution\n",
    "### Step_6. Resize images 2 times larger\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Just simply download the images we super-resolved."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step_7. Resize all images to 3840x2160 with Bicubic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python pre_ResizeSuperResolution.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pre_Data-Cleaning\n",
    "### Step_8. Filter out the ignored areas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python pre_FilterIgnoredAreas.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pre_Data-Slicing\n",
    "### Step_9. Slice Super Resolution images (3840x2160 --> 832x832)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create COCO format for slicing\n",
    "!python pre_FormatYolo2Coco.py  --path Dataset/SuperResolution_Training_Dataset\n",
    "\n",
    "!pytohn pre_Slicing.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pre_Data-Augmentation\n",
    "### Step_11. Augmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# move the code we change in MMDetection\n",
    "!cp Code/mm__init__.py mmdetection/mmdet/datasets/pipelines/__init__.py\n",
    "!cp Code/browse_dataset.py mmdetection/tools/misc/browse_dataset.py\n",
    "!cp Code/image.py mmdetection/mmdet/core/visualization/image.py\n",
    "!cp Code/SmallObjectAugmentation.py mmdetection/mmdet/datasets/pipelines/SmallObjectAugmentation.py\n",
    "!cp Code/transform.py mmdetection/ mmdetection/mmdet/datasets/pipelines/transform.py\n",
    "!cp Code/dataset_visualize.py mmdetection/dataset_visualize.py\n",
    "\n",
    "# install mmdetection\n",
    "!pip install -U openmim\n",
    "!mim install mmcv-full\n",
    "!cd mmdetection\n",
    "!pip install -v -e .\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists('../Dataset/SuperResolution_Training_Sliced_Augmentation'):\n",
    "    os.mkdir('../Dataset/SuperResolution_Training_Sliced_Augmentation')\n",
    "\n",
    "\n",
    "!python ./tools/misc/browse_dataset.py dataset_visualize.py --output-dir ../Dataset/SuperResolution_Training_Sliced_Augmentation/ --not-show\n",
    "\n",
    "!cd .."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# train_Model-832x832\n",
    "### Step_12-1. 832x832 model train\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('Start Splitting 10% of Training Data to Val Data...')\n",
    "\n",
    "TrainingDatasetRoot = 'Dataset/SuperResolution_Training_Sliced_Augmentation'\n",
    "os.mkdir(f'{TrainingDatasetRoot}train')\n",
    "os.mkdir(f'{TrainingDatasetRoot}train/images')\n",
    "os.mkdir(f'{TrainingDatasetRoot}train/labels')\n",
    "os.mkdir(f'{TrainingDatasetRoot}val')\n",
    "os.mkdir(f'{TrainingDatasetRoot}val/images')\n",
    "os.mkdir(f'{TrainingDatasetRoot}val/labels')\n",
    "val_index = random.sample(range(1, 1000), 100)\n",
    "for index in tqdm(val_index):\n",
    "    os.rename(f'{TrainingDatasetRoot}train/images/img{index:04d}.png',\n",
    "              f'{TrainingDatasetRoot}val/images/img{index:04d}.png')\n",
    "    os.rename(f'{TrainingDatasetRoot}train/labels/img{index:04d}.txt',\n",
    "              f'{TrainingDatasetRoot}val/labels/img{index:04d}.txt')\n",
    "\n",
    "!cd yolov5\n",
    "\n",
    "#training\n",
    "!python -m torch.distributed.run --nproc_per_node 2 train.py --weights yolov5l6.pt --cfg yolov5l6.yaml --hyp hyp.high_custom.yaml --data dataset-SuperResolution_Training_Sliced_Augmentation.yaml --epochs 1000 --batch-size 20 --imgsz 1664 --device 0,1 --patience 50"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step_12-2. 832x832 model detect (Public 0.724934 / Private 0.753749)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!cp -R Dataset/SuperResolution_Public_Dataset/*.png Dataset/SuperResolution_PublicPrivate_Dataset/\n",
    "\n",
    "!python detect.py --data dataset-SuperResolution_Training_Sliced_Augmentation.yaml --imgsz 3072 5376 --save-txt --save-conf --weight runs/train/exp2/weights/best.pt --source ../Dataset/SuperResolution_PublicPrivate_Dataset --augment --conf-thres 0.4\n",
    "# If you skip training you can simply run the following line\n",
    "# !python detect.py --data dataset-SuperResolution_Training_Sliced_Augmentation.yaml --imgsz 3072 5376 --save-txt --save-conf --weight Dataset/Models/832x832.pt --source ../Dataset/SuperResolution_PublicPrivate_Dataset --augment --conf-thres 0.4\n",
    "!cd .."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python Code/post_FormatYolo2Official.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step_12-2. 832x832 model detect WBF with Baseline model (Public 0.725854 / Private 0.754987)\n",
    "# This is a mysterious step, but it made the highest score !\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Detect the super-resolved public and private images with baseline model first\n",
    "!cd yolov5\n",
    "!python detect.py --data dataset-Origin_Training_Dataset.yaml --imgsz 3072 5376 --save-txt --save-conf --weight runs/train/exp/weights/best.pt --source ../Dataset/SuperResolution_PublicPrivate_Dataset --augment --conf-thres 0.4\n",
    "# If you skip training you can simply run the following line\n",
    "# !python detect.py --data dataset-Origin_Training_Dataset.yaml --imgsz 3072 5376 --save-txt --save-conf --weight Dataset/Models/baseline.pt --source ../Dataset/SuperResolution_PublicPrivate_Dataset --augment --conf-thres 0.4\n",
    "!cd .."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# WBF the result of 832x832 model and baseline model with weights of 1.3 : 1\n",
    "!python Code/post_WBF.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python Code/post_FormatYolo2Official.py\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
