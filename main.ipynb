{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# pre_Data-Format\n",
    "### Step_1. FormatOfficial2Yolo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python Code/pre_FormatOfficial2Yolo.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# train_Model-Baseline\n",
    "### Step_2-1. Baseline model train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# move the code we change in YOLOv5\n",
    "!cp Code/yolov5l6.yaml yolov5/yolov5l6.yaml\n",
    "!cp Code/dataset-Origin_Training_Dataset.yaml yolov5/dataset-Origin_Training_Dataset.yaml\n",
    "!cp Code/hyp.none.yaml yolov5/hyp.none.yaml\n",
    "\n",
    "!cd yolov5\n",
    "\n",
    "# training\n",
    "!python -m torch.distributed.run --nproc_per_node 2 train.py --weights yolov5l6.pt --cfg yolov5l6.yaml --hyp hyp.none.yaml --data dataset-Training_Dataset_v5.yaml --epochs 1000 --batch-size 8 --imgsz 1920 --device 0,1 --patience 50"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step_2-2. Baseline model detect (Public 0.70)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python detect.py --data dataset-Origin_Training_Dataset.yaml --imgsz 1080 1920 --save-txt --save-conf --weight runs/train/exp/weights/best.pt --source ../Dataset/public\n",
    "# If you skip training you can simply run the following line\n",
    "# !python detect.py --data dataset-Origin_Training_Dataset.yaml --imgsz 1080 1920 --save-txt --save-conf --weight Dataset/Models/baseline.pt --source ../Dataset/public\n",
    "!cd .."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python Code/post_FormatYolo2Official.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pre_Data-Cleaning\n",
    "### Step_3. Analyze bbox\n",
    "### Mistaken bbox"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python Code/pre_AnalyzeBBox_Mistaken.py\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.axis('off')\n",
    "plt.imshow(cv2.imread('Output/AnalyzeBBox_Mistaken.png'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outlier bbox\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python Code/pre_AnalyzeBBox_Outlier.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pre_Data-Classify\n",
    "### Step_4. Classify bbox with size\n",
    "### K-means"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python Code/pre_AnalyzeBBox_kmeans.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "Classes = ['car', 'hov', 'person', 'motorcycle']\n",
    "\n",
    "for Class in Classes:\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(cv2.imread(f'Output/AnalyzeBBox_3-means_{Class}.png'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pre_Data-Cleaning\n",
    "### Step_5. Compare GT bbox with baseline prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Just simply download the labels we already re-labeled."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pre_Data-SuperResolution\n",
    "### Step_6. Resize images 2 times larger\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Just simply download the images we super-resolved."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step_7. Resize all images to 3840x2160 with Bicubic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python pre_ResizeSuperResolution.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pre_Data-Cleaning\n",
    "### Step_8. Filter out the ignored areas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python pre_FilterIgnoredAreas.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pre_Data-Slicing\n",
    "### Step_9. Slice Super Resolution images (3840x2160 --> 832x832)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create COCO format for slicing\n",
    "!python pre_FormatYolo2Coco.py  --path Dataset/SuperResolution_Training_Dataset\n",
    "\n",
    "!pytohn pre_Slicing.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pre_Data-Augmentation\n",
    "### Step_11. Augmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# move the code we change in MMDetection\n",
    "!cp Code/mm__init__.py mmdetection/mmdet/datasets/pipelines/__init__.py\n",
    "!cp Code/browse_dataset.py mmdetection/tools/misc/browse_dataset.py\n",
    "!cp Code/image.py mmdetection/mmdet/core/visualization/image.py\n",
    "!cp Code/SmallObjectAugmentation.py mmdetection/mmdet/datasets/pipelines/SmallObjectAugmentation.py\n",
    "!cp Code/transform.py mmdetection/ mmdetection/mmdet/datasets/pipelines/transform.py\n",
    "!cp Code/dataset_visualize.py mmdetection/dataset_visualize.py\n",
    "\n",
    "# install mmdetection\n",
    "!pip install -U openmim\n",
    "!mim install mmcv-full\n",
    "!cd mmdetection\n",
    "!pip install -v -e .\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists('../Dataset/SuperResolution_Training_Sliced_Augmentation'):\n",
    "    os.mkdir('../Dataset/SuperResolution_Training_Sliced_Augmentation')\n",
    "\n",
    "\n",
    "!python ./tools/misc/browse_dataset.py dataset_visualize.py --output-dir ../Dataset/SuperResolution_Training_Sliced_Augmentation/ --not-show\n",
    "\n",
    "!cd .."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# train_Model-832x832\n",
    "### Step_12-1. 832x832 model train\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('Start Splitting 10% of Training Data to Val Data...')\n",
    "\n",
    "TrainingDatasetRoot = 'Dataset/SuperResolution_Training_Sliced_Augmentation'\n",
    "os.mkdir(f'{TrainingDatasetRoot}train')\n",
    "os.mkdir(f'{TrainingDatasetRoot}train/images')\n",
    "os.mkdir(f'{TrainingDatasetRoot}train/labels')\n",
    "os.mkdir(f'{TrainingDatasetRoot}val')\n",
    "os.mkdir(f'{TrainingDatasetRoot}val/images')\n",
    "os.mkdir(f'{TrainingDatasetRoot}val/labels')\n",
    "val_index = random.sample(range(1, 1000), 100)\n",
    "for index in tqdm(val_index):\n",
    "    os.rename(f'{TrainingDatasetRoot}train/images/img{index:04d}.png',\n",
    "              f'{TrainingDatasetRoot}val/images/img{index:04d}.png')\n",
    "    os.rename(f'{TrainingDatasetRoot}train/labels/img{index:04d}.txt',\n",
    "              f'{TrainingDatasetRoot}val/labels/img{index:04d}.txt')\n",
    "\n",
    "!cd yolov5\n",
    "\n",
    "#training\n",
    "!python -m torch.distributed.run --nproc_per_node 2 train.py --weights yolov5l6.pt --cfg yolov5l6.yaml --hyp hyp.high_custom.yaml --data dataset-SuperResolution_Training_Sliced_Augmentation.yaml --epochs 1000 --batch-size 20 --imgsz 1664 --device 0,1 --patience 50"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step_12-2. 832x832 model detect (Public 0.724934 / Private 0.753749)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!cp -R Dataset/SuperResolution_Public_Dataset/*.png Dataset/SuperResolution_PublicPrivate_Dataset/\n",
    "\n",
    "!python detect.py --data dataset-SuperResolution_Training_Sliced_Augmentation.yaml --imgsz 3072 5376 --save-txt --save-conf --weight runs/train/exp2/weights/best.pt --source ../Dataset/SuperResolution_PublicPrivate_Dataset --augment --conf-thres 0.4\n",
    "# If you skip training you can simply run the following line\n",
    "# !python detect.py --data dataset-SuperResolution_Training_Sliced_Augmentation.yaml --imgsz 3072 5376 --save-txt --save-conf --weight Dataset/Models/832x832.pt --source ../Dataset/SuperResolution_PublicPrivate_Dataset --augment --conf-thres 0.4\n",
    "!cd .."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python Code/post_FormatYolo2Official.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step_12-2. 832x832 model detect WBF with Baseline model (Public 0.725854 / Private 0.754987)\n",
    "# This is a mysterious step, but it made the highest score !\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Detect the super-resolved public and private images with baseline model first\n",
    "!cd yolov5\n",
    "!python detect.py --data dataset-Origin_Training_Dataset.yaml --imgsz 3072 5376 --save-txt --save-conf --weight runs/train/exp/weights/best.pt --source ../Dataset/SuperResolution_PublicPrivate_Dataset --augment --conf-thres 0.4\n",
    "# If you skip training you can simply run the following line\n",
    "# !python detect.py --data dataset-Origin_Training_Dataset.yaml --imgsz 3072 5376 --save-txt --save-conf --weight Dataset/Models/baseline.pt --source ../Dataset/SuperResolution_PublicPrivate_Dataset --augment --conf-thres 0.4\n",
    "!cd .."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# WBF the result of 832x832 model and baseline model with weights of 1.3 : 1\n",
    "!python Code/post_WBF.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python Code/post_FormatYolo2Official.py\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
